{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Offensive_Language_Hate_Speech.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB1e690C8Hx2"
   },
   "source": [
    "# Notebook to replicate the results of the experiment conducted on the Offensive Language and Hate Speech dataset by Davidson et al. (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmv5be0m8j8m"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "wqwDdI-ft8g3"
   },
   "source": [
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from tabulate import tabulate\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t14B1ur67Ywk"
   },
   "source": [
    "download and unzip glove.6B.100d.txt from Stanford University"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6-x1vDAlt-zO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "20aeab94-5500-4e1c-96c3-0c87ed337858"
   },
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2022-01-14 15:42:24--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2022-01-14 15:42:24--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-01-14 15:42:24--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 39s  \n",
      "\n",
      "2022-01-14 15:45:03 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCMw-aps7Csz"
   },
   "source": [
    "download labelled_data.npy from Github Repo of Towards Unbiased and Accurate Deferral to Multiple Experts by Keswani et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rE_QBfSf7AAl"
   },
   "source": [
    "#download from https://github.com/vijaykeswani/Deferral-To-Multiple-Experts/blob/3aa1f621991cb9d7823757d68c178870b686da62/output/labelled_data.npy"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APMPNoMm7kGp"
   },
   "source": [
    "#Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PwIyhR1rt8g4"
   },
   "source": [
    "NUM_CLASSES = 2\n",
    "DROPOUT = 0.00\n",
    "NUM_HIDDEN_UNITS = 50\n",
    "\n",
    "PATH_OLHS_DATA = 'labelled_data.npy'\n",
    "PATH_GLOVE_MODEL = 'glove.6B.100d.txt'\n",
    "\n",
    "NUM_EXPERTS = 20\n",
    "USE_LR_SCHEDULER = True\n",
    "\n",
    "TRAIN_BATCH_SIZE = 512\n",
    "TEST_BATCH_SIZE = 512\n",
    "LR = 5e-3\n",
    "EPOCHS = 20"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQTDL-9W8DFN"
   },
   "source": [
    "#Definition of Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjJyKxIa7mac"
   },
   "source": [
    "Classes for Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Glove Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Glove Model\")\n",
    "f = open(PATH_GLOVE_MODEL, 'r')\n",
    "GLOVE_MODEL = {}\n",
    "for line in f:\n",
    "    split_lines = line.split()\n",
    "    word = split_lines[0]\n",
    "    word_embedding = np.array([float(value) for value in split_lines[1:]])\n",
    "    GLOVE_MODEL[word] = word_embedding\n",
    "\n",
    "\n",
    "class OLHS_Dataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        self.post_ids = data[\"PostId\"].values\n",
    "        self.targets = data[\"Label\"].values\n",
    "        self.groups = data[\"Group\"].values\n",
    "\n",
    "        self.posts_features = []\n",
    "        for features in data[\"Feature\"].values:\n",
    "            features = torch.from_numpy(features).float()\n",
    "            features = features.to(device)\n",
    "            self.posts_features.append(features)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        post_id, target, group = self.post_ids[index], self.targets[index], self.groups[index]\n",
    "        post_features = self.posts_features[index]\n",
    "        return post_features, target, group, post_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.posts_features)\n",
    "\n",
    "class OLHS_3_Split_Dataloader:\n",
    "    def __init__(self, train_batch_size=128, test_batch_size=128, seed=42):\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "        vocab = GLOVE_MODEL.keys()\n",
    "\n",
    "        data = np.load(PATH_OLHS_DATA, allow_pickle=True)[()]\n",
    "        posts = data['posts']\n",
    "        labels = data['labels']\n",
    "        labels = [1 if int(l) < 2 else 0 for l in labels]\n",
    "        groups = data['groups']\n",
    "        groups = [int(g) for g in groups]\n",
    "\n",
    "        postsToFeatures = {}\n",
    "        for i, p in enumerate(posts):\n",
    "            feat = []\n",
    "            for w in p:\n",
    "                if w in vocab:\n",
    "                    feat.append(GLOVE_MODEL[w])\n",
    "            if len(feat) == 0:\n",
    "                continue\n",
    "\n",
    "            feat = np.mean(feat, axis=0)\n",
    "            postsToFeatures[i] = feat\n",
    "\n",
    "        all_indices = list(postsToFeatures.keys())\n",
    "        train_len = int(len(posts) * 0.8)\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        train_indices = np.random.choice(all_indices, train_len, replace=False)\n",
    "        val_test_indices = np.setdiff1d(all_indices, train_indices)\n",
    "\n",
    "        val_len = int(len(posts) * 0.1)\n",
    "        val_indices = np.random.choice(val_test_indices, val_len, replace=False)\n",
    "\n",
    "        test_indices = np.setdiff1d(val_test_indices, val_indices)\n",
    "\n",
    "        train_features = [postsToFeatures[i] for i in train_indices]\n",
    "        train_labels = [labels[i] for i in train_indices]\n",
    "        train_groups = [groups[i] for i in train_indices]\n",
    "\n",
    "        val_features = [postsToFeatures[i] for i in val_indices]\n",
    "        val_labels = [labels[i] for i in val_indices]\n",
    "        val_groups = [groups[i] for i in val_indices]\n",
    "\n",
    "        test_features = [postsToFeatures[i] for i in test_indices]\n",
    "        test_labels = [labels[i] for i in test_indices]\n",
    "        test_groups = [groups[i] for i in test_indices]\n",
    "\n",
    "        train_df = pd.DataFrame({\"PostId\": train_indices, \"Label\": train_labels, \"Group\": train_groups, \"Feature\": train_features})\n",
    "        val_df = pd.DataFrame({\"PostId\": val_indices, \"Label\": val_labels, \"Group\": val_groups, \"Feature\": val_features})\n",
    "        test_df = pd.DataFrame({\"PostId\": test_indices, \"Label\": test_labels, \"Group\": test_groups, \"Feature\": test_features})\n",
    "\n",
    "        self.trainset = OLHS_Dataset(train_df)\n",
    "        self.valset = OLHS_Dataset(val_df)\n",
    "        self.testset = OLHS_Dataset(test_df)\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        train_loader = self._get_data_loader(dataset=self.trainset, batch_size=self.train_batch_size, drop_last=True)\n",
    "        val_loader = self._get_data_loader(dataset=self.valset, batch_size=self.test_batch_size, drop_last=False)\n",
    "        test_loader = self._get_data_loader(dataset=self.testset, batch_size=self.test_batch_size, drop_last=False)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def _get_data_loader(self, dataset, batch_size, drop_last, shuffle=True):\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0, drop_last=drop_last)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PJX6uk-MwmY",
    "outputId": "635d6eac-744d-41ba-c53c-c61226b677f4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions for our loss and JSF loss"
   ],
   "metadata": {
    "collapsed": false,
    "id": "j-QodXdwMwnB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def joint_sparse_framework_loss(epoch, classifier_output, allocation_system_output, expert_preds, targets):\n",
    "    # Input:\n",
    "    #   epoch: int = current epoch (used for epoch-dependent weighting of allocation system loss)\n",
    "    #   classifier_output: softmax probabilities as class probabilities,  nxm matrix with n=batch size, m=number of classes\n",
    "    #   allocation_system_output: sigmoid outputs as expert weights,  nx(m+1) matrix with n=batch size, m=number of experts + 1 for machine\n",
    "    #   expert_preds: nxm matrix with expert predictions with n=number of experts, m=number of classes\n",
    "    #   targets: targets as 1-dim vector with n length with n=batch_size\n",
    "\n",
    "    # loss for allocation system \n",
    "\n",
    "    # set up zero-initialized tensor to store weighted team predictions\n",
    "    batch_size = len(targets)\n",
    "    weighted_team_preds = torch.zeros((batch_size, NUM_CLASSES)).to(classifier_output.device)\n",
    "\n",
    "    # for each team member add the weighted prediction to the team prediction\n",
    "    # start with machine\n",
    "    weighted_team_preds = weighted_team_preds + allocation_system_output[:, 0].reshape(-1, 1) * classifier_output\n",
    "    # continue with human experts\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[expert_preds[idx].astype(int)]).to(classifier_output.device)\n",
    "        weighted_team_preds = weighted_team_preds + allocation_system_output[:, idx + 1].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    # calculate team probabilities using softmax\n",
    "    team_probs = nn.Softmax(dim=1)(weighted_team_preds)\n",
    "\n",
    "    # alpha2 is 1-epoch^0.5 (0.5 taken from code of preprint paper) <--- used for experiments\n",
    "    alpha2 = 1 - (epoch ** -0.5)\n",
    "    alpha2 = torch.tensor(alpha2).to(classifier_output.device)\n",
    "\n",
    "    # weight the negative log likelihood loss with alpha2 to get team loss\n",
    "    log_team_probs = torch.log(team_probs + 1e-7)\n",
    "    allocation_system_loss = nn.NLLLoss(reduction=\"none\")(log_team_probs, targets.long())\n",
    "    allocation_system_loss = torch.mean(alpha2 * allocation_system_loss)\n",
    "\n",
    "    # loss for classifier\n",
    "\n",
    "    alpha1 = 1\n",
    "    log_classifier_output = torch.log(classifier_output + 1e-7)\n",
    "    classifier_loss = nn.NLLLoss(reduction=\"none\")(log_classifier_output, targets.long())\n",
    "    classifier_loss = alpha1 * torch.mean(classifier_loss)\n",
    "\n",
    "    # combine both losses\n",
    "    system_loss = classifier_loss + allocation_system_loss\n",
    "\n",
    "    return system_loss\n",
    "\n",
    "def our_loss(epoch, classifier_output, allocation_system_output, expert_preds, targets):\n",
    "    # Input:\n",
    "    #   epoch: int = current epoch (not used)\n",
    "    #   classifier_output: softmax probabilities as class probabilities,  nxm matrix with n=batch size, m=number of classes\n",
    "    #   allocation_system_output: softmax outputs as weights,  nx(m+1) matrix with n=batch size, m=number of experts + 1 for machine\n",
    "    #   expert_preds: nxm matrix with expert predictions with n=number of experts, m=number of classes\n",
    "    #   targets: targets as 1-dim vector with n length with n=batch_size\n",
    "\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(classifier_output.device) # set up zero-initialized tensor to store team predictions\n",
    "    team_probs = team_probs + allocation_system_output[:, 0].reshape(-1, 1) * classifier_output # add the weighted classifier prediction to the team prediction\n",
    "    for idx in range(NUM_EXPERTS): # continue with human experts\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[expert_preds[idx].astype(int)]).to(classifier_output.device)\n",
    "        team_probs = team_probs + allocation_system_output[:, idx + 1].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    system_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return system_loss\n",
    "\n",
    "def mixture_of_ai_experts_loss(allocation_system_output, classifiers_outputs, targets):\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(allocation_system_output.device)\n",
    "    classifiers_outputs = classifiers_outputs.to(allocation_system_output.device)\n",
    "\n",
    "    for idx in range(NUM_EXPERTS+1):\n",
    "        team_probs = team_probs + allocation_system_output[:, idx].reshape(-1, 1) * classifiers_outputs[idx]\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    moae_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return moae_loss\n",
    "\n",
    "def mixture_of_human_experts_loss(allocation_system_output, human_expert_preds, targets):\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(allocation_system_output.device)\n",
    "\n",
    "    # human experts\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[human_expert_preds[idx].astype(int)]).to(allocation_system_output.device)\n",
    "        team_probs = team_probs + allocation_system_output[:, idx].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    mohe_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return mohe_loss"
   ],
   "metadata": {
    "id": "3W-5iSvQMwnF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Class for classifier and allocation system network"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_w45YT0MMwnJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, output_size, softmax_sigmoid=\"softmax\"):\n",
    "        super().__init__()\n",
    "        self.softmax_sigmoid = softmax_sigmoid\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(100, NUM_HIDDEN_UNITS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_HIDDEN_UNITS, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        output = self.classifier(features)\n",
    "        if self.softmax_sigmoid == \"softmax\":\n",
    "            output = nn.Softmax(dim=1)(output)\n",
    "        elif self.softmax_sigmoid == \"sigmoid\":\n",
    "            output = nn.Sigmoid()(output)\n",
    "        return output"
   ],
   "metadata": {
    "id": "b7zsGeK2MwnK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classes and Functions for Experts"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZK_5_XsvMwnL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def flip(p):\n",
    "    return 1 if random.random() < p else 0\n",
    "\n",
    "class Expert:\n",
    "    def __init__(self, pq, post_indices, labels, groups):\n",
    "        self.pq = pq\n",
    "\n",
    "        self.preds = {}\n",
    "        for group, label, post_idx in zip(groups, labels, post_indices):\n",
    "            toss = flip(self.pq[int(group)])\n",
    "            pred = int(label) if toss else 1 - int(label)\n",
    "            self.preds[post_idx] = pred\n",
    "\n",
    "    def predict(self, post_indices):\n",
    "        predictions = [self.preds[post_idx] for post_idx in post_indices]\n",
    "        return predictions\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Expert accuracies:\" + str(round(self.p[0], 2)) + \", \" + str(round(self.p[0], 2))\n",
    "\n",
    "class AverageExpert:\n",
    "    def __init__(self, expert_fns=[]):\n",
    "        self.expert_fns = expert_fns\n",
    "        self.num_experts = len(self.expert_fns)\n",
    "\n",
    "    def predict(self, post_indices):\n",
    "        random.shuffle(self.expert_fns)\n",
    "        all_experts_predictions = [expert_fn(post_indices) for expert_fn in self.expert_fns]\n",
    "        predictions = [None] * len(post_indices)\n",
    "\n",
    "        for idx, expert_predictions in enumerate(all_experts_predictions):\n",
    "            predictions[idx::self.num_experts] = expert_predictions[idx::self.num_experts]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def get_uniform_experts(num, min_prob=0.5, toggle=0):\n",
    "    data = np.load(PATH_OLHS_DATA, allow_pickle=True)[()]\n",
    "    labels = data['labels']\n",
    "    labels = [1 if int(l) < 2 else 0 for l in labels]\n",
    "    groups = data['groups']\n",
    "    groups = [int(g) for g in groups]\n",
    "    post_indices = list(range(0,len(labels)))\n",
    "\n",
    "    experts = []\n",
    "    for i in range(num):\n",
    "        p = np.random.uniform(min_prob, 1)\n",
    "        q = np.random.uniform(min_prob, p)\n",
    "        if toggle:\n",
    "            p, q = q, p\n",
    "\n",
    "        experts.append(Expert((p, q), post_indices, labels, groups))\n",
    "    return experts"
   ],
   "metadata": {
    "id": "64scEQKhMwnO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions for Metric Calculation"
   ],
   "metadata": {
    "collapsed": false,
    "id": "z5rexjhXMwnR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_accuracy(preds, targets):\n",
    "    if len(targets) > 0:\n",
    "        acc = accuracy_score(targets, preds)\n",
    "    else:\n",
    "        acc = 0\n",
    "\n",
    "    return acc\n",
    "\n",
    "def get_coverage(task_subset_targets, targets):\n",
    "    num_images = len(targets)\n",
    "    num_images_in_task_subset = len(task_subset_targets)\n",
    "    coverage = num_images_in_task_subset / num_images\n",
    "\n",
    "    return coverage\n",
    "\n",
    "def get_classifier_metrics(classifier_preds, allocation_system_decisions, targets):\n",
    "    # classifier performance on all tasks\n",
    "    classifier_accuracy = get_accuracy(classifier_preds, targets)\n",
    "\n",
    "    # filter for subset of tasks that are allocated to the classifier\n",
    "    task_subset = (allocation_system_decisions == 0)\n",
    "\n",
    "    # classifier performance on those tasks\n",
    "    task_subset_classifier_preds = classifier_preds[task_subset]\n",
    "    task_subset_targets = targets[task_subset]\n",
    "    classifier_task_subset_accuracy = get_accuracy(task_subset_classifier_preds, task_subset_targets)\n",
    "\n",
    "    # coverage\n",
    "    classifier_coverage = get_coverage(task_subset_targets, targets)\n",
    "\n",
    "    return classifier_accuracy, classifier_task_subset_accuracy, classifier_coverage\n",
    "\n",
    "def get_experts_metrics(expert_preds, allocation_system_decisions, targets):\n",
    "    expert_accuracies = []\n",
    "    expert_task_subset_accuracies = []\n",
    "    expert_coverages = []\n",
    "\n",
    "    # calculate metrics for each expert\n",
    "    for expert_idx in range(NUM_EXPERTS):\n",
    "\n",
    "        # expert performance on all tasks\n",
    "        preds = expert_preds[expert_idx]\n",
    "        expert_accuracy = get_accuracy(preds, targets)\n",
    "\n",
    "        # filter for subset of tasks that are allocated to the expert with number \"idx\"\n",
    "        task_subset = (allocation_system_decisions == expert_idx+1)\n",
    "\n",
    "        # expert performance on tasks assigned by allocation system\n",
    "        task_subset_expert_preds = preds[task_subset]\n",
    "        task_subset_targets = targets[task_subset]\n",
    "        expert_task_subset_accuracy = get_accuracy(task_subset_expert_preds, task_subset_targets)\n",
    "\n",
    "        # coverage\n",
    "        expert_coverage = get_coverage(task_subset_targets, targets)\n",
    "\n",
    "        expert_accuracies.append(expert_accuracy)\n",
    "        expert_task_subset_accuracies.append(expert_task_subset_accuracy)\n",
    "        expert_coverages.append(expert_coverage)\n",
    "\n",
    "    return expert_accuracies, expert_task_subset_accuracies, expert_coverages\n",
    "\n",
    "def get_metrics(epoch, allocation_system_outputs, classifier_outputs, expert_preds, targets, loss_fn):\n",
    "    metrics = {}\n",
    "\n",
    "    # Metrics for system\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    classifier_preds = np.argmax(classifier_outputs, 1)\n",
    "    preds = np.vstack((classifier_preds, expert_preds)).T\n",
    "    system_preds = preds[range(len(preds)), allocation_system_decisions.astype(int)]\n",
    "    system_accuracy = get_accuracy(system_preds, targets)\n",
    "\n",
    "    system_loss = loss_fn(epoch=epoch,\n",
    "                          classifier_output=torch.tensor(classifier_outputs).float(),\n",
    "                          allocation_system_output=torch.tensor(allocation_system_outputs).float(),\n",
    "                          expert_preds=expert_preds,\n",
    "                          targets=torch.tensor(targets).long())\n",
    "\n",
    "    metrics[\"System Accuracy\"] = system_accuracy\n",
    "    metrics[\"System Loss\"] = system_loss\n",
    "\n",
    "    # Metrics for classifier\n",
    "    classifier_accuracy, classifier_task_subset_accuracy, classifier_coverage = get_classifier_metrics(classifier_preds, allocation_system_decisions, targets)\n",
    "    metrics[\"Classifier Accuracy\"] = classifier_accuracy\n",
    "    metrics[\"Classifier Task Subset Accuracy\"] = classifier_task_subset_accuracy\n",
    "    metrics[\"Classifier Coverage\"] = classifier_coverage\n",
    "\n",
    "    # Metrics for experts \n",
    "    \"\"\"expert_accuracies, experts_task_subset_accuracies, experts_coverages = get_experts_metrics(expert_preds, allocation_system_decisions, targets)\n",
    "\n",
    "    for expert_idx, (expert_accuracy, expert_task_subset_accuracy, expert_coverage) in enumerate(zip(expert_accuracies, experts_task_subset_accuracies, experts_coverages)):\n",
    "        metrics[f'Expert {expert_idx+1} Accuracy'] = expert_accuracy\n",
    "        metrics[f'Expert {expert_idx+1} Task Subset Accuracy'] = expert_task_subset_accuracy\n",
    "        metrics[f'Expert {expert_idx+1} Coverage'] = expert_coverage\"\"\"\n",
    "\n",
    "    return system_accuracy, system_loss, metrics"
   ],
   "metadata": {
    "id": "Naxh9jt5MwnS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions for Training and Evaluation of Our Approach and JSF"
   ],
   "metadata": {
    "collapsed": false,
    "id": "65aA7obgMwnU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn):\n",
    "    classifier.train()\n",
    "    allocation_system.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(train_loader):\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        expert_batch_preds = np.empty((NUM_EXPERTS, len(batch_targets)))\n",
    "        for idx, expert_fn in enumerate(expert_fns):\n",
    "            expert_batch_preds[idx] = np.array(expert_fn(batch_post_ids.numpy()))\n",
    "\n",
    "        batch_outputs_classifier = classifier(batch_input)\n",
    "        batch_outputs_allocation_system = allocation_system(batch_input)\n",
    "\n",
    "        batch_loss = loss_fn(epoch=epoch, classifier_output=batch_outputs_classifier, allocation_system_output=batch_outputs_allocation_system,\n",
    "                                expert_preds=expert_batch_preds, targets=batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_one_epoch(epoch, classifier, allocation_system, data_loader, expert_fns, loss_fn):\n",
    "    classifier.eval()\n",
    "    allocation_system.eval()\n",
    "\n",
    "    classifier_outputs = torch.tensor([]).to(device)\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).long().to(device)\n",
    "    post_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(data_loader):\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "\n",
    "            batch_classifier_outputs = classifier(batch_input)\n",
    "            classifier_outputs = torch.cat((classifier_outputs, batch_classifier_outputs))\n",
    "\n",
    "            batch_allocation_system_outputs = allocation_system(batch_input)\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "\n",
    "            post_ids.extend(batch_post_ids.numpy())\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(post_ids))\n",
    "\n",
    "    classifier_outputs = classifier_outputs.cpu().numpy()\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    system_accuracy, system_loss, metrics = get_metrics(epoch, allocation_system_outputs, classifier_outputs, expert_preds, targets, loss_fn)\n",
    "\n",
    "    return system_accuracy, system_loss, metrics\n",
    "\n",
    "def run_team_performance_optimization(method, seed, expert_fns):\n",
    "    print(f'Training multi expert deferral with {method}')\n",
    "\n",
    "    if method == \"Joint Sparse Framework\":\n",
    "        loss_fn = joint_sparse_framework_loss\n",
    "        allocation_system_activation_function = \"sigmoid\"\n",
    "\n",
    "\n",
    "    elif method == \"Our Approach\":\n",
    "        loss_fn = our_loss\n",
    "        allocation_system_activation_function = \"softmax\"\n",
    "\n",
    "    classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    allocation_system = Network(output_size=NUM_EXPERTS + 1,\n",
    "                                 softmax_sigmoid=allocation_system_activation_function).to(device)\n",
    "\n",
    "    ohs_dl = OLHS_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed)\n",
    "    train_loader, val_loader, test_loader = ohs_dl.get_data_loader()\n",
    "\n",
    "    parameters = list(classifier.parameters()) + list(allocation_system.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_accuracy = 0\n",
    "    best_val_system_loss = 100\n",
    "    best_metrics = None\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        train_one_epoch(epoch, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn)\n",
    "\n",
    "        val_system_accuracy, val_system_loss, _ = evaluate_one_epoch(epoch, classifier, allocation_system, val_loader, expert_fns, loss_fn)\n",
    "        _, _, test_metrics = evaluate_one_epoch(epoch, classifier, allocation_system, test_loader, expert_fns, loss_fn)\n",
    "\n",
    "        if method == \"Joint Sparse Framework\":\n",
    "            if val_system_accuracy > best_val_system_accuracy:\n",
    "                best_val_system_accuracy = val_system_accuracy\n",
    "                best_metrics = test_metrics\n",
    "\n",
    "        elif method == \"Our Approach\":\n",
    "            if val_system_loss < best_val_system_loss:\n",
    "                best_val_system_loss = val_system_loss\n",
    "                best_metrics = test_metrics\n",
    "\n",
    "    print(f'\\n Earlystopping Results for {method}:')\n",
    "    system_metrics_keys = [key for key in best_metrics.keys() if \"System\" in key]\n",
    "    for k in system_metrics_keys:\n",
    "        print(f'\\t {k}: {best_metrics[k]}')\n",
    "    print()\n",
    "\n",
    "    classifier_metrics_keys = [key for key in best_metrics.keys() if \"Classifier\" in key]\n",
    "    for k in classifier_metrics_keys:\n",
    "        print(f'\\t {k}: {best_metrics[k]}')\n",
    "    print()\n",
    "\n",
    "    \"\"\"for exp_idx in range(NUM_EXPERTS):\n",
    "      expert_metrics_keys = [key for key in best_metrics.keys() if f'Expert {exp_idx+1} ' in key]\n",
    "      for k in expert_metrics_keys:\n",
    "          print(f'\\t {k}: {best_metrics[k]}')\n",
    "    print()\"\"\"\n",
    "\n",
    "    return best_metrics[\"System Accuracy\"], best_metrics[\"Classifier Coverage\"]"
   ],
   "metadata": {
    "id": "mvkeB0m4MwnW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions for Evaluation of Baselines"
   ],
   "metadata": {
    "collapsed": false,
    "id": "t20oU01PMwnZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_accuracy_of_best_expert(seed, expert_fns):\n",
    "    ohs_dl = OLHS_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed)\n",
    "    _, _, test_loader = ohs_dl.get_data_loader()\n",
    "\n",
    "    targets = torch.tensor([]).long()\n",
    "    post_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (_, batch_targets, batch_groups, batch_post_ids) in enumerate(test_loader):\n",
    "            batch_targets = batch_targets\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "\n",
    "            post_ids.extend(batch_post_ids.numpy())\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(post_ids))\n",
    "\n",
    "    expert_accuracies = []\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        preds = expert_preds[idx]\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        expert_accuracies.append(acc)\n",
    "\n",
    "    print(f'Best Expert Accuracy: {max(expert_accuracies)}\\n')\n",
    "\n",
    "    return max(expert_accuracies)\n",
    "\n",
    "def get_accuracy_of_average_expert(seed, expert_fns):\n",
    "    ohs_dl = OLHS_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed)\n",
    "    _, _, test_loader = ohs_dl.get_data_loader()\n",
    "\n",
    "    targets = torch.tensor([]).long()\n",
    "    post_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (_, batch_targets, batch_groups, batch_post_ids) in enumerate(test_loader):\n",
    "            batch_targets = batch_targets\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "\n",
    "            post_ids.extend(batch_post_ids.numpy())\n",
    "\n",
    "\n",
    "    avg_expert = AverageExpert(expert_fns)\n",
    "    avg_expert_preds = avg_expert.predict(post_ids)\n",
    "    avg_expert_acc = accuracy_score(targets, avg_expert_preds)\n",
    "    print(f'Average Expert Accuracy: {avg_expert_acc}\\n')\n",
    "\n",
    "    return avg_expert_acc"
   ],
   "metadata": {
    "id": "IzIobIcqMwna"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions for Training and Evaluation of Full Automation Baseline"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8l0PDKFeMwnl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train_full_automation_one_epoch(classifier, train_loader, optimizer, scheduler):\n",
    "    # switch to train mode\n",
    "    classifier.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(train_loader):\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        batch_outputs_classifier = classifier(batch_input)\n",
    "        log_output = torch.log(batch_outputs_classifier + 1e-7)\n",
    "        batch_loss = nn.NLLLoss()(log_output, batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "def evaluate_full_automation_one_epoch(classifier, data_loader):\n",
    "    classifier.eval()\n",
    "\n",
    "    classifier_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(data_loader):\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "\n",
    "            batch_classifier_outputs = classifier(batch_input)\n",
    "            classifier_outputs = torch.cat((classifier_outputs, batch_classifier_outputs))\n",
    "\n",
    "    log_output = torch.log(classifier_outputs + 1e-7)\n",
    "    full_automation_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    classifier_outputs = classifier_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    classifier_preds = np.argmax(classifier_outputs, 1)\n",
    "    full_automation_accuracy = get_accuracy(classifier_preds, targets)\n",
    "\n",
    "    return full_automation_accuracy, full_automation_loss\n",
    "\n",
    "def run_full_automation(seed):\n",
    "    print(f'Training full automation baseline')\n",
    "\n",
    "    classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    ohs_dl = OLHS_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed)\n",
    "    train_loader, val_loader, test_loader = ohs_dl.get_data_loader()\n",
    "\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_loss = 100\n",
    "    best_test_system_accuracy = None\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        train_full_automation_one_epoch(classifier, train_loader, optimizer, scheduler)\n",
    "\n",
    "        val_system_accuracy, val_system_loss = evaluate_full_automation_one_epoch(classifier, val_loader)\n",
    "        test_system_accuracy, test_system_loss, = evaluate_full_automation_one_epoch(classifier, test_loader)\n",
    "\n",
    "        if val_system_loss < best_val_system_loss:\n",
    "            best_val_system_loss = val_system_loss\n",
    "            best_test_system_accuracy = test_system_accuracy\n",
    "\n",
    "    print(f'Full Automation Accuracy: {best_test_system_accuracy}\\n')\n",
    "    return best_test_system_accuracy\n"
   ],
   "metadata": {
    "id": "LEcBeUnxMwnm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions for Training and Evaluation of Mixture of Artificial Experts Baseline"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8sOV_nhjMwno"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train_moae_one_epoch(classifiers, allocation_system, train_loader, optimizer, scheduler):\n",
    "    # switch to train mode\n",
    "    allocation_system.train()\n",
    "    for classifier in classifiers:\n",
    "        classifier.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(train_loader):\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        batch_outputs_allocation_system = allocation_system(batch_input)\n",
    "        batch_outputs_classifiers = torch.empty((NUM_EXPERTS+1, len(batch_targets), NUM_CLASSES))\n",
    "        for idx, classifier in enumerate(classifiers):\n",
    "            batch_outputs_classifiers[idx] = classifier(batch_input)\n",
    "\n",
    "        # compute and record loss\n",
    "        batch_loss = mixture_of_ai_experts_loss(allocation_system_output=batch_outputs_allocation_system,\n",
    "                                                   classifiers_outputs=batch_outputs_classifiers, targets=batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_moae_one_epoch(classifiers, allocation_system, data_loader):\n",
    "    allocation_system.eval()\n",
    "    for classifier in classifiers:\n",
    "        classifier.eval()\n",
    "\n",
    "    classifiers_outputs = torch.tensor([]).to(device)\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(data_loader):\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_allocation_system_outputs = allocation_system(batch_input)\n",
    "            batch_outputs_classifiers = torch.empty((NUM_EXPERTS+1, len(batch_targets), NUM_CLASSES)).to(device)\n",
    "            for idx, classifier in enumerate(classifiers):\n",
    "                batch_outputs_classifiers[idx] = classifier(batch_input)\n",
    "\n",
    "            classifiers_outputs = torch.cat((classifiers_outputs, batch_outputs_classifiers), dim=1)\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets.float()))\n",
    "\n",
    "    moae_loss = mixture_of_ai_experts_loss(allocation_system_output=allocation_system_outputs,\n",
    "                                                   classifiers_outputs=classifiers_outputs, targets=targets.long())\n",
    "\n",
    "    classifiers_outputs = classifiers_outputs.cpu().numpy()\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    classifiers_preds = np.argmax(classifiers_outputs, 2).T\n",
    "    team_preds = classifiers_preds[range(len(classifiers_preds)), allocation_system_decisions.astype(int)]\n",
    "    moae_accuracy = get_accuracy(team_preds, targets)\n",
    "\n",
    "    return moae_accuracy, moae_loss\n",
    "\n",
    "def run_moae(seed):\n",
    "    print(f'Training Mixture of artificial experts baseline')\n",
    "\n",
    "    allocation_system = Network(output_size=NUM_EXPERTS + 1,\n",
    "                                 softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    classifiers = []\n",
    "    for _ in range(NUM_EXPERTS+1):\n",
    "        classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "        classifiers.append(classifier)\n",
    "\n",
    "    ohs_dl = OLHS_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed)\n",
    "    train_loader, val_loader, test_loader = ohs_dl.get_data_loader()\n",
    "\n",
    "    parameters = list(allocation_system.parameters())\n",
    "    for classifier in classifiers:\n",
    "        parameters += list(classifier.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_loss = 100\n",
    "    best_test_system_accuracy = None\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        train_moae_one_epoch(classifiers, allocation_system, train_loader, optimizer, scheduler)\n",
    "        val_moae_accuracy, val_moae_loss = evaluate_moae_one_epoch(classifiers, allocation_system, val_loader)\n",
    "        test_moae_accuracy, test_moae_loss = evaluate_moae_one_epoch(classifiers, allocation_system, test_loader)\n",
    "\n",
    "        if val_moae_loss < best_val_system_loss:\n",
    "            best_val_system_loss = val_moae_loss\n",
    "            best_test_system_accuracy = test_moae_accuracy\n",
    "\n",
    "    print(f'Mixture of Artificial Experts Accuracy: {best_test_system_accuracy}\\n')\n",
    "    return best_test_system_accuracy\n"
   ],
   "metadata": {
    "id": "f5e5UmCcMwnp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions for Training and Evaluation of Mixture of Human Experts Baseline"
   ],
   "metadata": {
    "collapsed": false,
    "id": "bH4fE7LNMwnr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def train_mohe_one_epoch(allocation_system, train_loader, optimizer, scheduler, expert_fns):\n",
    "    # switch to train mode\n",
    "    allocation_system.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(train_loader):\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        expert_batch_preds = np.empty((NUM_EXPERTS, len(batch_targets)))\n",
    "        for idx, expert_fn in enumerate(expert_fns):\n",
    "            expert_batch_preds[idx] = np.array(expert_fn(batch_post_ids.numpy()))\n",
    "\n",
    "        batch_outputs_allocation_system = allocation_system(batch_input)\n",
    "\n",
    "        # compute and record loss\n",
    "        batch_loss = mixture_of_human_experts_loss(allocation_system_output=batch_outputs_allocation_system,\n",
    "                                                   human_expert_preds=expert_batch_preds, targets=batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "def evaluate_mohe_one_epoch(allocation_system, data_loader, expert_fns):\n",
    "    allocation_system.eval()\n",
    "\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).to(device)\n",
    "    post_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_groups, batch_post_ids) in enumerate(data_loader):\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            batch_allocation_system_outputs = allocation_system(batch_input)\n",
    "\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "            post_ids.extend(batch_post_ids.numpy())\n",
    "\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(post_ids))\n",
    "\n",
    "    # compute and record loss\n",
    "    mohe_loss = mixture_of_human_experts_loss(allocation_system_output=allocation_system_outputs,\n",
    "                                                   human_expert_preds=expert_preds, targets=targets.long())\n",
    "\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    expert_preds = expert_preds.T\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    team_preds = expert_preds[range(len(expert_preds)), allocation_system_decisions.astype(int)]\n",
    "    mohe_accuracy = get_accuracy(team_preds, targets)\n",
    "\n",
    "    return mohe_accuracy, mohe_loss\n",
    "\n",
    "\n",
    "def run_mohe(seed, expert_fns):\n",
    "    print(f'Training Mixture of human experts baseline')\n",
    "\n",
    "    allocation_system = Network(output_size=NUM_EXPERTS,\n",
    "                                 softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    ohs_dl = OLHS_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed)\n",
    "    train_loader, val_loader, test_loader = ohs_dl.get_data_loader()\n",
    "\n",
    "    parameters = allocation_system.parameters()\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_loss = 100\n",
    "    best_test_system_accuracy = None\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        train_mohe_one_epoch(allocation_system, train_loader, optimizer, scheduler, expert_fns)\n",
    "        val_mohe_accuracy, val_mohe_loss = evaluate_mohe_one_epoch(allocation_system, val_loader, expert_fns)\n",
    "        test_mohe_accuracy, test_mohe_loss = evaluate_mohe_one_epoch(allocation_system, test_loader, expert_fns)\n",
    "\n",
    "        if val_mohe_loss < best_val_system_loss:\n",
    "            best_val_system_loss = val_mohe_loss\n",
    "            best_test_system_accuracy = test_mohe_accuracy\n",
    "\n",
    "    print(f'Mixture of Human Experts Accuracy: {best_test_system_accuracy}\\n')\n",
    "    return best_test_system_accuracy\n",
    "\n"
   ],
   "metadata": {
    "id": "C0bx_a-VMwnt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Run Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "id": "R05rE-03Mwnw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Seed: 0\n",
      "----------------------------------------\n",
      "Num AAE Experts: 15, Num Non-AAE Experts: 5\n",
      "\n",
      "Best Expert Accuracy: 0.9646053702196908\n",
      "\n",
      "Average Expert Accuracy: 0.7628152969894223\n",
      "\n",
      "Training multi expert deferral with Our Approach\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.72it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Earlystopping Results for Our Approach:\n",
      "\t System Accuracy: 0.96826688364524\n",
      "\t System Loss: 0.061494549465643686\n",
      "\n",
      "\t Classifier Accuracy: 0.838893409275834\n",
      "\t Classifier Task Subset Accuracy: 0\n",
      "\t Classifier Coverage: 0.0\n",
      "\n",
      "Training multi expert deferral with Joint Sparse Framework\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Earlystopping Results for Joint Sparse Framework:\n",
      "\t System Accuracy: 0.9174125305126118\n",
      "\t System Loss: 0.25251489671342237\n",
      "\n",
      "\t Classifier Accuracy: 0.8909682668836453\n",
      "\t Classifier Task Subset Accuracy: 1.0\n",
      "\t Classifier Coverage: 0.0028478437754271765\n",
      "\n",
      "Training Mixture of human experts baseline\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.83it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mixture of Human Experts Accuracy: 0.9707078925956062\n",
      "\n",
      "Training full automation baseline\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.13it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full Automation Accuracy: 0.8966639544344996\n",
      "\n",
      "Training Mixture of artificial experts baseline\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mixture of Artificial Experts Accuracy: 0.8938161106590724\n",
      "\n",
      "----------------------------------------\n",
      "Seed: 1\n",
      "----------------------------------------\n",
      "Num AAE Experts: 15, Num Non-AAE Experts: 5\n",
      "\n",
      "Best Expert Accuracy: 0.9320585842148088\n",
      "\n",
      "Average Expert Accuracy: 0.7310821806346623\n",
      "\n",
      "Training multi expert deferral with Our Approach\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.71it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Earlystopping Results for Our Approach:\n",
      "\t System Accuracy: 0.9694873881204231\n",
      "\t System Loss: 0.0584044598287273\n",
      "\n",
      "\t Classifier Accuracy: 0.8287225386493083\n",
      "\t Classifier Task Subset Accuracy: 0.9894736842105263\n",
      "\t Classifier Coverage: 0.5410903173311635\n",
      "\n",
      "Training multi expert deferral with Joint Sparse Framework\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Earlystopping Results for Joint Sparse Framework:\n",
      "\t System Accuracy: 0.9328722538649309\n",
      "\t System Loss: 0.287274813367474\n",
      "\n",
      "\t Classifier Accuracy: 0.8816110659072417\n",
      "\t Classifier Task Subset Accuracy: 1.0\n",
      "\t Classifier Coverage: 0.0020341741253051262\n",
      "\n",
      "Training Mixture of human experts baseline\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mixture of Human Experts Accuracy: 0.934092758340114\n",
      "\n",
      "Training full automation baseline\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full Automation Accuracy: 0.8954434499593165\n",
      "\n",
      "Training Mixture of artificial experts baseline\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.15it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mixture of Artificial Experts Accuracy: 0.8995117982099268\n",
      "\n",
      "----------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_expert_accuracies = []\n",
    "avg_expert_accuracies = []\n",
    "our_approach_accuracies = []\n",
    "our_approach_coverages = []\n",
    "jsf_accuracies = []\n",
    "jsf_coverages = []\n",
    "full_automation_accuracies = []\n",
    "mohe_accuracies = []\n",
    "moae_accuracies = []\n",
    "\n",
    "for seed in range(20):\n",
    "    print(f'Seed: {seed}')\n",
    "    print(\"-\"*40)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    min_prob = 0.6\n",
    "    num_experts_aae = math.floor(NUM_EXPERTS * 3 / 4)\n",
    "    num_experts_non_aae = NUM_EXPERTS - num_experts_aae\n",
    "    aae_experts = get_uniform_experts(num_experts_aae, min_prob)\n",
    "    non_aae_experts = get_uniform_experts(num_experts_non_aae, min_prob, toggle=1)\n",
    "\n",
    "    experts = []\n",
    "    experts.extend(aae_experts)\n",
    "    experts.extend(non_aae_experts)\n",
    "    expert_fns = [expert.predict for expert in experts]\n",
    "    print(f'Num AAE Experts: {num_experts_aae}, Num Non-AAE Experts: {num_experts_non_aae}\\n')\n",
    "\n",
    "    best_expert_accuracy = get_accuracy_of_best_expert(seed, expert_fns)\n",
    "    best_expert_accuracies.append(best_expert_accuracy)\n",
    "    \n",
    "    avg_expert_accuracy = get_accuracy_of_average_expert(seed, expert_fns)\n",
    "    avg_expert_accuracies.append(avg_expert_accuracy)\n",
    "\n",
    "    our_approach_accuracy, our_approach_coverage = run_team_performance_optimization(\"Our Approach\", seed, expert_fns)\n",
    "    our_approach_accuracies.append(our_approach_accuracy)\n",
    "    our_approach_coverages.append(our_approach_coverage)\n",
    "    \n",
    "    jsf_accuracy, jsf_coverage = run_team_performance_optimization(\"Joint Sparse Framework\", seed, expert_fns)\n",
    "    jsf_accuracies.append(jsf_accuracy)\n",
    "    jsf_coverages.append(jsf_coverage)\n",
    "    \n",
    "    mohe_accuracy = run_mohe(seed, expert_fns)\n",
    "    mohe_accuracies.append(mohe_accuracy)\n",
    "\n",
    "    full_automation_accuracy = run_full_automation(seed)\n",
    "    full_automation_accuracies.append(full_automation_accuracy)\n",
    "    \n",
    "    moae_accuracy = run_moae(seed)\n",
    "    moae_accuracies.append(moae_accuracy)\n",
    "    print(\"-\"*40)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoRr0J5DMwny",
    "outputId": "0da6b9c2-f13e-4136-d9fb-0b4ab196a126"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "mean_best_expert_accuracy = np.mean(best_expert_accuracies)\n",
    "mean_best_expert_coverage = 0.00\n",
    "\n",
    "mean_avg_expert_accuracy = np.mean(avg_expert_accuracies)\n",
    "mean_avg_expert_coverage = 0.00\n",
    "\n",
    "mean_our_approach_accuracy = np.mean(our_approach_accuracies)\n",
    "mean_our_approach_coverage = np.mean(our_approach_coverages)\n",
    "\n",
    "mean_jsf_accuracy = np.mean(jsf_accuracies)\n",
    "mean_jsf_coverage = np.mean(jsf_coverages)\n",
    "\n",
    "mean_full_automation_accuracy = np.mean(full_automation_accuracies)\n",
    "mean_full_automation_coverage = 100.00\n",
    "\n",
    "mean_moae_accuracy = np.mean(moae_accuracies)\n",
    "mean_moae_coverage = 100.00\n",
    "\n",
    "mean_mohe_accuracy = np.mean(mohe_accuracies)\n",
    "mean_mohe_coverage = 0.00\n",
    "\n"
   ],
   "metadata": {
    "id": "8qGOm3R6Mwn2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Method           Accuracy            Coverage\n",
      "---------------  ------------------  ---------------------\n",
      "Our Approach     0.9688771358828316  0.27054515866558176\n",
      "JSF              0.9251423921887714  0.0024410089503661514\n",
      "--------         --------            --------\n",
      "Full Automation  0.896053702196908   100.0\n",
      "Random Expert    0.7469487388120424  0.0\n",
      "Best Expert      0.9483319772172498  0.0\n",
      "MOHE             0.9524003254678601  0.0\n",
      "MOAE             0.8966639544344996  100.0\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([['Our Approach', mean_our_approach_accuracy, mean_our_approach_coverage],\n",
    "                ['JSF', mean_jsf_accuracy, mean_jsf_coverage],\n",
    "                ['--------', '--------', '--------'],\n",
    "                ['Full Automation', mean_full_automation_accuracy, mean_full_automation_coverage],\n",
    "                ['Random Expert', mean_avg_expert_accuracy, mean_avg_expert_coverage],\n",
    "                ['Best Expert', mean_best_expert_accuracy, mean_best_expert_coverage], \n",
    "                ['MOHE', mean_mohe_accuracy, mean_mohe_coverage],\n",
    "                ['MOAE', mean_moae_accuracy, mean_moae_coverage]],\n",
    "               headers=['Method', 'Accuracy', 'Coverage']))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zW6VIeGoMwn8",
    "outputId": "f49e57bf-8a64-4810-a364-f971fe823f4b"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "spSgPuqOTGNz"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
