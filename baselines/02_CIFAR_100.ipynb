{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB1e690C8Hx2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notebook to replicate the results of the experiments conducted on the CIFAR-100 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmv5be0m8j8m",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wqwDdI-ft8g3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APMPNoMm7kGp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PwIyhR1rt8g4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 20\n",
    "DROPOUT = 0.00\n",
    "NUM_HIDDEN_UNITS = 100\n",
    "LR = 5e-3\n",
    "USE_LR_SCHEDULER = False\n",
    "TRAIN_BATCH_SIZE = 512\n",
    "TEST_BATCH_SIZE = 512\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQTDL-9W8DFN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Definition of Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjJyKxIa7mac",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Classes for Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KArzJS-bjcsq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR100_Dataset(torchvision.datasets.CIFAR100):\n",
    "    def __getitem__(self, index: int):\n",
    "        img, fine_target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(fine_target)\n",
    "\n",
    "        return img, target, fine_target\n",
    "\n",
    "class CIFAR100_3_Split_Dataloader:\n",
    "    def __init__(self, train_batch_size=128, test_batch_size=128, seed=42, small_version=False):\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.seed = seed\n",
    "        self.small_version = small_version\n",
    "\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5071, 0.4867, 0.4408],\n",
    "                                 [0.2675, 0.2565, 0.2761])])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5071, 0.4867, 0.4408],\n",
    "                                 [0.2675, 0.2565, 0.2761])])\n",
    "\n",
    "        coarse_labels = np.array([\n",
    "            4, 1, 14, 8, 0, 6, 7, 7, 18, 3,\n",
    "            3, 14, 9, 18, 7, 11, 3, 9, 7, 11,\n",
    "            6, 11, 5, 10, 7, 6, 13, 15, 3, 15,\n",
    "            0, 11, 1, 10, 12, 14, 16, 9, 11, 5,\n",
    "            5, 19, 8, 8, 15, 13, 14, 17, 18, 10,\n",
    "            16, 4, 17, 4, 2, 0, 17, 4, 18, 17,\n",
    "            10, 3, 2, 12, 12, 16, 12, 1, 9, 19,\n",
    "            2, 10, 0, 1, 16, 12, 9, 13, 15, 13,\n",
    "            16, 19, 2, 4, 6, 19, 5, 5, 8, 19,\n",
    "            18, 1, 2, 15, 6, 0, 17, 8, 14, 13])\n",
    "\n",
    "        target_transform = lambda x: coarse_labels[x]\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        train_val_set = CIFAR100_Dataset(root='./data', train=True, download=True, transform=transform_train, target_transform=target_transform)\n",
    "        all_indices = np.arange(0, 50000, 1)\n",
    "        train_indices = np.random.choice(all_indices, 40000, replace=False)\n",
    "        val_indices = np.setdiff1d(all_indices, train_indices)\n",
    "        self.trainset = torch.utils.data.Subset(train_val_set, train_indices)\n",
    "        self.valset = torch.utils.data.Subset(train_val_set, val_indices)\n",
    "\n",
    "        self.testset = CIFAR100_Dataset(root='./data', train=False, download=True, transform=transform_test, target_transform=target_transform)\n",
    "\n",
    "        if self.small_version:\n",
    "            np.random.seed(self.seed)\n",
    "            train_indices = np.random.choice(np.arange(0, 40000, 1), 4000, replace=False)\n",
    "            val_indices = np.random.choice(np.arange(0, 10000, 1), 1000, replace=False)\n",
    "            test_indices = np.random.choice(np.arange(0, 10000, 1), 1000, replace=False)\n",
    "\n",
    "            self.trainset = torch.utils.data.Subset(self.trainset, train_indices)\n",
    "            self.valset = torch.utils.data.Subset(self.valset, val_indices)\n",
    "            self.testset = torch.utils.data.Subset(self.testset, test_indices)\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        train_loader = self._get_data_loader(dataset=self.trainset, batch_size=self.train_batch_size, drop_last=True)\n",
    "        val_loader = self._get_data_loader(dataset=self.valset, batch_size=self.test_batch_size, drop_last=False)\n",
    "        test_loader = self._get_data_loader(dataset=self.testset, batch_size=self.test_batch_size, drop_last=False)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def _get_data_loader(self, dataset, batch_size, drop_last, shuffle=True):\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2, drop_last=drop_last, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4qxVJZkjcsr"
   },
   "source": [
    "Functions for our loss and JSF loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-s72UAWpjcss",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def joint_sparse_framework_loss(epoch, classifier_output, allocation_system_output, expert_preds, targets):\n",
    "    # Input:\n",
    "    #   epoch: int = current epoch (used for epoch-dependent weighting of allocation system loss)\n",
    "    #   classifier_output: softmax probabilities as class probabilities,  nxm matrix with n=batch size, m=number of classes\n",
    "    #   allocation_system_output: sigmoid outputs as expert weights,  nx(m+1) matrix with n=batch size, m=number of experts + 1 for machine\n",
    "    #   expert_preds: nxm matrix with expert predictions with n=number of experts, m=number of classes\n",
    "    #   targets: targets as 1-dim vector with n length with n=batch_size\n",
    "\n",
    "    # loss for allocation system \n",
    "\n",
    "    # set up zero-initialized tensor to store weighted team predictions\n",
    "    batch_size = len(targets)\n",
    "    weighted_team_preds = torch.zeros((batch_size, NUM_CLASSES)).to(classifier_output.device)\n",
    "\n",
    "    # for each team member add the weighted prediction to the team prediction\n",
    "    # start with machine\n",
    "    weighted_team_preds = weighted_team_preds + allocation_system_output[:, 0].reshape(-1, 1) * classifier_output\n",
    "    # continue with human experts\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[expert_preds[idx].astype(int)]).to(classifier_output.device)\n",
    "        weighted_team_preds = weighted_team_preds + allocation_system_output[:, idx + 1].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    # calculate team probabilities using softmax\n",
    "    team_probs = nn.Softmax(dim=1)(weighted_team_preds)\n",
    "\n",
    "    # alpha2 is 1-epoch^0.5 (0.5 taken from code of preprint paper) <--- used for experiments\n",
    "    alpha2 = 1 - (epoch ** -0.5)\n",
    "    alpha2 = torch.tensor(alpha2).to(classifier_output.device)\n",
    "\n",
    "    # weight the negative log likelihood loss with alpha2 to get team loss\n",
    "    log_team_probs = torch.log(team_probs + 1e-7)\n",
    "    allocation_system_loss = nn.NLLLoss(reduction=\"none\")(log_team_probs, targets.long())\n",
    "    allocation_system_loss = torch.mean(alpha2 * allocation_system_loss)\n",
    "\n",
    "    # loss for classifier\n",
    "\n",
    "    alpha1 = 1\n",
    "    log_classifier_output = torch.log(classifier_output + 1e-7)\n",
    "    classifier_loss = nn.NLLLoss(reduction=\"none\")(log_classifier_output, targets.long())\n",
    "    classifier_loss = alpha1 * torch.mean(classifier_loss)\n",
    "\n",
    "    # combine both losses\n",
    "    system_loss = classifier_loss + allocation_system_loss\n",
    "\n",
    "    return system_loss\n",
    "\n",
    "def our_loss(epoch, classifier_output, allocation_system_output, expert_preds, targets):\n",
    "    # Input:\n",
    "    #   epoch: int = current epoch (not used)\n",
    "    #   classifier_output: softmax probabilities as class probabilities,  nxm matrix with n=batch size, m=number of classes\n",
    "    #   allocation_system_output: softmax outputs as weights,  nx(m+1) matrix with n=batch size, m=number of experts + 1 for machine\n",
    "    #   expert_preds: nxm matrix with expert predictions with n=number of experts, m=number of classes\n",
    "    #   targets: targets as 1-dim vector with n length with n=batch_size\n",
    "\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(classifier_output.device) # set up zero-initialized tensor to store team predictions\n",
    "    team_probs = team_probs + allocation_system_output[:, 0].reshape(-1, 1) * classifier_output # add the weighted classifier prediction to the team prediction\n",
    "    for idx in range(NUM_EXPERTS): # continue with human experts\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[expert_preds[idx].astype(int)]).to(classifier_output.device)\n",
    "        team_probs = team_probs + allocation_system_output[:, idx + 1].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    system_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return system_loss\n",
    "\n",
    "def mixture_of_ai_experts_loss(allocation_system_output, classifiers_outputs, targets):\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(allocation_system_output.device)\n",
    "    classifiers_outputs = classifiers_outputs.to(allocation_system_output.device)\n",
    "\n",
    "    for idx in range(NUM_EXPERTS+1):\n",
    "        team_probs = team_probs + allocation_system_output[:, idx].reshape(-1, 1) * classifiers_outputs[idx]\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    moae_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return moae_loss\n",
    "\n",
    "def mixture_of_human_experts_loss(allocation_system_output, human_expert_preds, targets):\n",
    "    batch_size = len(targets)\n",
    "    team_probs = torch.zeros((batch_size, NUM_CLASSES)).to(allocation_system_output.device)\n",
    "\n",
    "    # human experts\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        one_hot_expert_preds = torch.tensor(np.eye(NUM_CLASSES)[human_expert_preds[idx].astype(int)]).to(allocation_system_output.device)\n",
    "        team_probs = team_probs + allocation_system_output[:, idx].reshape(-1, 1) * one_hot_expert_preds\n",
    "\n",
    "    log_output = torch.log(team_probs + 1e-7)\n",
    "    mohe_loss = nn.NLLLoss()(log_output, targets)\n",
    "\n",
    "    return mohe_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23EMRGUujcst"
   },
   "source": [
    "Class for classifier and allocation system network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "16aj3uShjcsu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        del self.resnet.fc\n",
    "\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.training = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        x = self.resnet.avgpool(x)\n",
    "        features = torch.flatten(x, 1)\n",
    "        return features\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, output_size, softmax_sigmoid=\"softmax\"):\n",
    "        super().__init__()\n",
    "        self.softmax_sigmoid = softmax_sigmoid\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, NUM_HIDDEN_UNITS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(NUM_HIDDEN_UNITS, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        output = self.classifier(features)\n",
    "        if self.softmax_sigmoid == \"softmax\":\n",
    "            output = nn.Softmax(dim=1)(output)\n",
    "        elif self.softmax_sigmoid == \"sigmoid\":\n",
    "            output = nn.Sigmoid()(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YunYs7Rjcsu"
   },
   "source": [
    "Classes and Functions for Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Eldab7TJjcsu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Cifar100Expert:\n",
    "    \"\"\"A class used to represent an Expert on CIFAR100 data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strengths : list[int]\n",
    "        list of subclass indices defining the experts strengths\n",
    "    weaknesses : list[int]\n",
    "        list of subclass indices defining the experts weaknesses\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    strengths : list[int]\n",
    "        list of subclass indices defining the experts strengths. If the subclass index of an image is in strength the expert makes a correct prediction, if not expert predicts a random superclass\n",
    "    weaknesses : list[int]\n",
    "        list of subclass indices defining the experts weaknesses. If the subclass index of an image is in weaknesses the expert predicts a random superclass, if not the expert makes a correct prediction\n",
    "    use_strengths : bool\n",
    "        a boolean indicating whether the expert is defined by its strengths or its weaknesses. True if strengths are not empty, False if strengths are empty\n",
    "    subclass_idx_to_superclass_idx : dict of {int : int}\n",
    "        a dictionary that maps the 100 subclass indices of CIFAR100 to their 20 superclass indices\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    predict(fine_ids)\n",
    "        makes a prediction based on the specified strengths or weaknesses and the given subclass indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, strengths: list = [], weaknesses: list = []):\n",
    "        self.strengths = strengths\n",
    "        self.weaknesses = weaknesses\n",
    "\n",
    "        assert len(self.strengths) > 0 or len(\n",
    "            self.weaknesses) > 0, \"the competence of a Cifar100Expert needs to be specified using either strengths or weaknesses\"\n",
    "\n",
    "        self.use_strengths = len(self.strengths) > 0\n",
    "\n",
    "        self.subclass_idx_to_superclass_idx = {0: 4, 1: 1, 2: 14, 3: 8, 4: 0, 5: 6, 6: 7, 7: 7, 8: 18, 9: 3, 10: 3, 11: 14, 12: 9, 13: 18, 14: 7,\n",
    "                                               15: 11, 16: 3, 17: 9, 18: 7, 19: 11,\n",
    "                                               20: 6, 21: 11, 22: 5, 23: 10, 24: 7, 25: 6, 26: 13, 27: 15, 28: 3, 29: 15, 30: 0, 31: 11, 32: 1,\n",
    "                                               33: 10, 34: 12, 35: 14, 36: 16, 37: 9,\n",
    "                                               38: 11, 39: 5, 40: 5, 41: 19, 42: 8, 43: 8, 44: 15, 45: 13, 46: 14, 47: 17, 48: 18, 49: 10, 50: 16,\n",
    "                                               51: 4, 52: 17, 53: 4, 54: 2, 55: 0,\n",
    "                                               56: 17, 57: 4, 58: 18, 59: 17, 60: 10, 61: 3, 62: 2, 63: 12, 64: 12, 65: 16, 66: 12, 67: 1, 68: 9,\n",
    "                                               69: 19, 70: 2, 71: 10, 72: 0, 73: 1,\n",
    "                                               74: 16, 75: 12, 76: 9, 77: 13, 78: 15, 79: 13, 80: 16, 81: 19, 82: 2, 83: 4, 84: 6, 85: 19, 86: 5,\n",
    "                                               87: 5, 88: 8, 89: 19, 90: 18, 91: 1,\n",
    "                                               92: 2, 93: 15, 94: 6, 95: 0, 96: 17, 97: 8, 98: 14, 99: 13}\n",
    "\n",
    "    def predict(self, subclass_idxs: list) -> list:\n",
    "        \"\"\"Predicts the superclass indices for the given subclass indices\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        subclass_idxs : list of int\n",
    "            list of subclass indices to get a prediction for. Predictions are made based on the specified strengths or weaknesses.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of int\n",
    "            returns a list of superclass indices that represent the experts prediction\n",
    "\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        if self.use_strengths:\n",
    "            for subclass_idx in subclass_idxs:\n",
    "                if subclass_idx in self.strengths:\n",
    "                    predictions.append(self.subclass_idx_to_superclass_idx[subclass_idx.item()])\n",
    "                else:\n",
    "                    predictions.append(random.randint(0, 19))\n",
    "        else:\n",
    "            for subclass_idx in subclass_idxs:\n",
    "                if subclass_idx in self.weaknesses:\n",
    "                    predictions.append(random.randint(0, 19))\n",
    "                else:\n",
    "                    predictions.append(self.subclass_idx_to_superclass_idx[subclass_idx.item()])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class Cifar100AverageExpert:\n",
    "    \"\"\"A class used to represent a cohort of Cifar100Experts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        expert_fns : list[Cifar100Expert.predict]\n",
    "            list of Cifar100Expert.predict functions that return the predictions of a Cifar100Expert for given subclass_idxs\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        expert_fns : list[Cifar100Expert.predict]\n",
    "            list of Cifar100Expert.predict functions that return the predictions of a Cifar100Expert for given subclass_idxs\n",
    "        num_experts : int\n",
    "            the number of experts in the cohort. Is the length of expert_fns\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        predict(subclass_idxs)\n",
    "            makes a prediction for the given subclass indices\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, expert_fns=[]):\n",
    "        self.expert_fns = expert_fns\n",
    "        self.num_experts = len(self.expert_fns)\n",
    "\n",
    "    def predict(self, subclass_idxs):\n",
    "        \"\"\"Returns the predictions of a random Cifar100Expert for each image for the given subclass indices\n",
    "\n",
    "        The first expert in expert_fns predicts the first image in subclass_idx.\n",
    "        The second expert in expert_fns predicts the second image in subclass_idx.\n",
    "        ...\n",
    "        If all experts in expert_fns made their prediction for one image, the first expert starts again.\n",
    "        If three experts are defined in expert_fns, the first expert predicts the 1st, 4th, 7th, 10th ... image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        subclass_idxs : list of int\n",
    "            list of subclass indices to get a prediction for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of int\n",
    "            returns a list of superclass indices that represent the experts prediction\n",
    "        \"\"\"\n",
    "        all_experts_predictions = [expert_fn(subclass_idxs) for expert_fn in self.expert_fns]\n",
    "        predictions = [None] * len(subclass_idxs)\n",
    "\n",
    "        for idx, expert_predictions in enumerate(all_experts_predictions):\n",
    "            predictions[idx::self.num_experts] = expert_predictions[idx::self.num_experts]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4Dc3laXjcsv"
   },
   "source": [
    "Functions for Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3dVHQL-Wjcsv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(preds, targets):\n",
    "    if len(targets) > 0:\n",
    "        acc = accuracy_score(targets, preds)\n",
    "    else:\n",
    "        acc = 0\n",
    "\n",
    "    return acc\n",
    "\n",
    "def get_coverage(task_subset_targets, targets):\n",
    "    num_images = len(targets)\n",
    "    num_images_in_task_subset = len(task_subset_targets)\n",
    "    coverage = num_images_in_task_subset / num_images\n",
    "\n",
    "    return coverage\n",
    "\n",
    "def get_classifier_metrics(classifier_preds, allocation_system_decisions, targets):\n",
    "    # classifier performance on all tasks\n",
    "    classifier_accuracy = get_accuracy(classifier_preds, targets)\n",
    "\n",
    "    # filter for subset of tasks that are allocated to the classifier\n",
    "    task_subset = (allocation_system_decisions == 0)\n",
    "\n",
    "    # classifier performance on those tasks\n",
    "    task_subset_classifier_preds = classifier_preds[task_subset]\n",
    "    task_subset_targets = targets[task_subset]\n",
    "    classifier_task_subset_accuracy = get_accuracy(task_subset_classifier_preds, task_subset_targets)\n",
    "\n",
    "    # coverage\n",
    "    classifier_coverage = get_coverage(task_subset_targets, targets)\n",
    "\n",
    "    return classifier_accuracy, classifier_task_subset_accuracy, classifier_coverage\n",
    "\n",
    "def get_experts_metrics(expert_preds, allocation_system_decisions, targets):\n",
    "    expert_accuracies = []\n",
    "    expert_task_subset_accuracies = []\n",
    "    expert_coverages = []\n",
    "\n",
    "    # calculate metrics for each expert\n",
    "    for expert_idx in range(NUM_EXPERTS):\n",
    "\n",
    "        # expert performance on all tasks\n",
    "        preds = expert_preds[expert_idx]\n",
    "        expert_accuracy = get_accuracy(preds, targets)\n",
    "\n",
    "        # filter for subset of tasks that are allocated to the expert with number \"idx\"\n",
    "        task_subset = (allocation_system_decisions == expert_idx+1)\n",
    "\n",
    "        # expert performance on tasks assigned by allocation system\n",
    "        task_subset_expert_preds = preds[task_subset]\n",
    "        task_subset_targets = targets[task_subset]\n",
    "        expert_task_subset_accuracy = get_accuracy(task_subset_expert_preds, task_subset_targets)\n",
    "\n",
    "        # coverage\n",
    "        expert_coverage = get_coverage(task_subset_targets, targets)\n",
    "\n",
    "        expert_accuracies.append(expert_accuracy)\n",
    "        expert_task_subset_accuracies.append(expert_task_subset_accuracy)\n",
    "        expert_coverages.append(expert_coverage)\n",
    "\n",
    "    return expert_accuracies, expert_task_subset_accuracies, expert_coverages\n",
    "\n",
    "def get_metrics(epoch, allocation_system_outputs, classifier_outputs, expert_preds, targets, loss_fn):\n",
    "    metrics = {}\n",
    "\n",
    "    # Metrics for system\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    classifier_preds = np.argmax(classifier_outputs, 1)\n",
    "    preds = np.vstack((classifier_preds, expert_preds)).T\n",
    "    system_preds = preds[range(len(preds)), allocation_system_decisions.astype(int)]\n",
    "    system_accuracy = get_accuracy(system_preds, targets)\n",
    "\n",
    "    system_loss = loss_fn(epoch=epoch,\n",
    "                          classifier_output=torch.tensor(classifier_outputs).float(),\n",
    "                          allocation_system_output=torch.tensor(allocation_system_outputs).float(),\n",
    "                          expert_preds=expert_preds,\n",
    "                          targets=torch.tensor(targets).long())\n",
    "\n",
    "    metrics[\"System Accuracy\"] = system_accuracy\n",
    "    metrics[\"System Loss\"] = system_loss\n",
    "\n",
    "    # Metrics for classifier\n",
    "    classifier_accuracy, classifier_task_subset_accuracy, classifier_coverage = get_classifier_metrics(classifier_preds, allocation_system_decisions, targets)\n",
    "    metrics[\"Classifier Accuracy\"] = classifier_accuracy\n",
    "    metrics[\"Classifier Task Subset Accuracy\"] = classifier_task_subset_accuracy\n",
    "    metrics[\"Classifier Coverage\"] = classifier_coverage\n",
    "\n",
    "    # Metrics for experts \n",
    "    \"\"\"expert_accuracies, experts_task_subset_accuracies, experts_coverages = get_experts_metrics(expert_preds, allocation_system_decisions, targets)\n",
    "\n",
    "    for expert_idx, (expert_accuracy, expert_task_subset_accuracy, expert_coverage) in enumerate(zip(expert_accuracies, experts_task_subset_accuracies, experts_coverages)):\n",
    "        metrics[f'Expert {expert_idx+1} Accuracy'] = expert_accuracy\n",
    "        metrics[f'Expert {expert_idx+1} Task Subset Accuracy'] = expert_task_subset_accuracy\n",
    "        metrics[f'Expert {expert_idx+1} Coverage'] = expert_coverage\"\"\"\n",
    "\n",
    "    return system_accuracy, system_loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YF7ldvYMjcsw"
   },
   "source": [
    "Functions for Training and Evaluation of Our Approach and JSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wVr4UTxbjcsw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, feature_extractor, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn):\n",
    "    feature_extractor.eval()\n",
    "    classifier.train()\n",
    "    allocation_system.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_subclass_idxs) in enumerate(train_loader):\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        expert_batch_preds = np.empty((NUM_EXPERTS, len(batch_targets)))\n",
    "        for idx, expert_fn in enumerate(expert_fns):\n",
    "            expert_batch_preds[idx] = np.array(expert_fn(batch_subclass_idxs))\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_classifier = classifier(batch_features)\n",
    "        batch_outputs_allocation_system = allocation_system(batch_features)\n",
    "\n",
    "        batch_loss = loss_fn(epoch=epoch, classifier_output=batch_outputs_classifier, allocation_system_output=batch_outputs_allocation_system,\n",
    "                                expert_preds=expert_batch_preds, targets=batch_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, data_loader, expert_fns, loss_fn):\n",
    "    feature_extractor.eval()\n",
    "    classifier.eval()\n",
    "    allocation_system.eval()\n",
    "\n",
    "    classifier_outputs = torch.tensor([]).to(device)\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).to(device)\n",
    "    subclass_idxs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_subclass_idxs) in enumerate(data_loader):\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_classifier_outputs = classifier(batch_features)\n",
    "            batch_allocation_system_outputs = allocation_system(batch_features)\n",
    "\n",
    "            classifier_outputs = torch.cat((classifier_outputs, batch_classifier_outputs))\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "            subclass_idxs.extend(batch_subclass_idxs)\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(subclass_idxs))\n",
    "\n",
    "    classifier_outputs = classifier_outputs.cpu().numpy()\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    system_accuracy, system_loss, metrics = get_metrics(epoch, allocation_system_outputs, classifier_outputs, expert_preds, targets, loss_fn)\n",
    "\n",
    "    return system_accuracy, system_loss, metrics\n",
    "\n",
    "def run_team_performance_optimization(method, seed, expert_fns):\n",
    "    print(f'Team Performance Optimization with {method}')\n",
    "\n",
    "    if method == \"Joint Sparse Framework\":\n",
    "        loss_fn = joint_sparse_framework_loss\n",
    "        allocation_system_activation_function = \"sigmoid\"\n",
    "\n",
    "\n",
    "    elif method == \"Our Approach\":\n",
    "        loss_fn = our_loss\n",
    "        allocation_system_activation_function = \"softmax\"\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    allocation_system = Network(output_size=NUM_EXPERTS + 1,\n",
    "                                 softmax_sigmoid=allocation_system_activation_function).to(device)\n",
    "\n",
    "    cifar_dl = CIFAR100_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed, small_version=False)\n",
    "    train_loader, val_loader, test_loader = cifar_dl.get_data_loader()\n",
    "\n",
    "    parameters = list(classifier.parameters()) + list(allocation_system.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_accuracy = 0\n",
    "    best_val_system_loss = 100\n",
    "    best_metrics = None\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        train_one_epoch(epoch, feature_extractor, classifier, allocation_system, train_loader, optimizer, scheduler, expert_fns, loss_fn)\n",
    "\n",
    "        val_system_accuracy, val_system_loss, _ = evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, val_loader, expert_fns, loss_fn)\n",
    "        _, _, test_metrics = evaluate_one_epoch(epoch, feature_extractor, classifier, allocation_system, test_loader, expert_fns, loss_fn)\n",
    "\n",
    "        if method == \"Joint Sparse Framework\":\n",
    "            if val_system_accuracy > best_val_system_accuracy:\n",
    "                best_val_system_accuracy = val_system_accuracy\n",
    "                best_metrics = test_metrics\n",
    "\n",
    "        elif method == \"Our Approach\":\n",
    "            if val_system_loss < best_val_system_loss:\n",
    "                best_val_system_loss = val_system_loss\n",
    "                best_metrics = test_metrics\n",
    "\n",
    "    print(f'\\n Earlystopping Results for {method}:')\n",
    "    system_metrics_keys = [key for key in best_metrics.keys() if \"System\" in key]\n",
    "    for k in system_metrics_keys:\n",
    "        print(f'\\t {k}: {best_metrics[k]}')\n",
    "    print()\n",
    "\n",
    "    classifier_metrics_keys = [key for key in best_metrics.keys() if \"Classifier\" in key]\n",
    "    for k in classifier_metrics_keys:\n",
    "        print(f'\\t {k}: {best_metrics[k]}')\n",
    "    print()\n",
    "\n",
    "    \"\"\"for exp_idx in range(NUM_EXPERTS):\n",
    "      expert_metrics_keys = [key for key in best_metrics.keys() if f'Expert {exp_idx+1} ' in key]\n",
    "      for k in expert_metrics_keys:\n",
    "          print(f'\\t {k}: {best_metrics[k]}')\n",
    "    print()\"\"\"\n",
    "\n",
    "    return best_metrics[\"System Accuracy\"], best_metrics[\"Classifier Coverage\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHkf2wxgjcsw"
   },
   "source": [
    "Functions for Evaluation of Human Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FHK8Yk48jcsx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy_of_best_expert(seed, expert_fns):\n",
    "    cifar_dl = CIFAR100_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed, small_version=False)\n",
    "    _, _, test_loader = cifar_dl.get_data_loader()\n",
    "\n",
    "    targets = torch.tensor([]).long()\n",
    "    subclass_idxs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (_, batch_targets, batch_subclass_idxs) in enumerate(test_loader):\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "            subclass_idxs.extend(batch_subclass_idxs)\n",
    "\n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(subclass_idxs))\n",
    "\n",
    "    expert_accuracies = []\n",
    "    for idx in range(NUM_EXPERTS):\n",
    "        preds = expert_preds[idx]\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        expert_accuracies.append(acc)\n",
    "\n",
    "    print(f'Best Expert Accuracy: {max(expert_accuracies)}\\n')\n",
    "\n",
    "    return max(expert_accuracies)\n",
    "\n",
    "def get_accuracy_of_average_expert(seed, expert_fns):\n",
    "    cifar_dl = CIFAR100_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed, small_version=False)\n",
    "    _, _, test_loader = cifar_dl.get_data_loader()\n",
    "\n",
    "    targets = torch.tensor([]).long()\n",
    "    subclass_idxs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (_, batch_targets, batch_subclass_idxs) in enumerate(test_loader):\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "            subclass_idxs.extend(batch_subclass_idxs)\n",
    "\n",
    "    avg_expert = Cifar100AverageExpert(expert_fns)\n",
    "    avg_expert_preds = avg_expert.predict(subclass_idxs)\n",
    "    avg_expert_acc = accuracy_score(targets, avg_expert_preds)\n",
    "    print(f'Average Expert Accuracy: {avg_expert_acc}\\n')\n",
    "\n",
    "    return avg_expert_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reZuentAjcsx"
   },
   "source": [
    "Functions for Training and Evaluation of Full Automation Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6eXEmsK1jcsx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_full_automation_one_epoch(feature_extractor, classifier, train_loader, optimizer, scheduler):\n",
    "    # switch to train mode\n",
    "    feature_extractor.eval()\n",
    "    classifier.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, _) in enumerate(train_loader):\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_classifier = classifier(batch_features)\n",
    "\n",
    "        log_output = torch.log(batch_outputs_classifier + 1e-7)\n",
    "        batch_loss = nn.NLLLoss()(log_output, batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_full_automation_one_epoch(feature_extractor, classifier, data_loader):\n",
    "    feature_extractor.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    classifier_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).to(device)\n",
    "    filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_filenames) in enumerate(data_loader):\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_classifier_outputs = classifier(batch_features)\n",
    "\n",
    "            classifier_outputs = torch.cat((classifier_outputs, batch_classifier_outputs))\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "            filenames.extend(batch_filenames)\n",
    "\n",
    "    log_output = torch.log(classifier_outputs + 1e-7)\n",
    "    full_automation_loss = nn.NLLLoss()(log_output, targets.long())\n",
    "\n",
    "    classifier_outputs = classifier_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    classifier_preds = np.argmax(classifier_outputs, 1)\n",
    "    full_automation_accuracy = get_accuracy(classifier_preds, targets)\n",
    "\n",
    "    return full_automation_accuracy, full_automation_loss\n",
    "\n",
    "def run_full_automation(seed):\n",
    "    print(f'Training full automation baseline')\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    cifar_dl = CIFAR100_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed, small_version=False)\n",
    "    train_loader, val_loader, test_loader = cifar_dl.get_data_loader()\n",
    "\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_loss = 100\n",
    "    best_test_system_accuracy = None\n",
    "\n",
    "    for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "        train_full_automation_one_epoch(feature_extractor, classifier, train_loader, optimizer, scheduler)\n",
    "\n",
    "        val_system_accuracy, val_system_loss = evaluate_full_automation_one_epoch(feature_extractor, classifier, val_loader)\n",
    "        test_system_accuracy, test_system_loss, = evaluate_full_automation_one_epoch(feature_extractor, classifier, test_loader)\n",
    "\n",
    "        if val_system_loss < best_val_system_loss:\n",
    "            best_val_system_loss = val_system_loss\n",
    "            best_test_system_accuracy = test_system_accuracy\n",
    "\n",
    "    print(f'Full Automation Accuracy: {best_test_system_accuracy}\\n')\n",
    "    return best_test_system_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC-cUd_A1PlD"
   },
   "source": [
    "Functions for Training and Evaluation of Mixture of Artificial Experts Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gXuLzbK81PlE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_moae_one_epoch(feature_extractor, classifiers, allocation_system, train_loader, optimizer, scheduler):\n",
    "    # switch to train mode\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.train()\n",
    "    for classifier in classifiers:\n",
    "        classifier.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, _) in enumerate(train_loader):\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_allocation_system = allocation_system(batch_features)\n",
    "        batch_outputs_classifiers = torch.empty((NUM_EXPERTS+1, len(batch_targets), NUM_CLASSES))\n",
    "        for idx, classifier in enumerate(classifiers):\n",
    "            batch_outputs_classifiers[idx] = classifier(batch_features)\n",
    "\n",
    "        # compute and record loss\n",
    "        batch_loss = mixture_of_ai_experts_loss(allocation_system_output=batch_outputs_allocation_system,\n",
    "                                                   classifiers_outputs=batch_outputs_classifiers, targets=batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "def evaluate_moae_one_epoch(feature_extractor, classifiers, allocation_system, data_loader):\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.eval()\n",
    "    for classifier in classifiers:\n",
    "        classifier.eval()\n",
    "\n",
    "    classifiers_outputs = torch.tensor([]).to(device)\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, _) in enumerate(data_loader):\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_allocation_system_outputs = allocation_system(batch_features)\n",
    "            batch_outputs_classifiers = torch.empty((NUM_EXPERTS+1, len(batch_targets), NUM_CLASSES)).to(device)\n",
    "            for idx, classifier in enumerate(classifiers):\n",
    "                batch_outputs_classifiers[idx] = classifier(batch_features)\n",
    "\n",
    "            classifiers_outputs = torch.cat((classifiers_outputs, batch_outputs_classifiers), dim=1)\n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets.float()))\n",
    "\n",
    "    moae_loss = mixture_of_ai_experts_loss(allocation_system_output=allocation_system_outputs,\n",
    "                                                   classifiers_outputs=classifiers_outputs, targets=targets.long())\n",
    "\n",
    "    classifiers_outputs = classifiers_outputs.cpu().numpy()\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    classifiers_preds = np.argmax(classifiers_outputs, 2).T\n",
    "    team_preds = classifiers_preds[range(len(classifiers_preds)), allocation_system_decisions.astype(int)]\n",
    "    moae_accuracy = get_accuracy(team_preds, targets)\n",
    "\n",
    "    return moae_accuracy, moae_loss\n",
    "\n",
    "def run_moae(seed):\n",
    "    print(f'Training Mixture of artificial experts baseline')\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    allocation_system = Network(output_size=NUM_EXPERTS + 1,\n",
    "                                 softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    classifiers = []\n",
    "    for _ in range(NUM_EXPERTS+1):\n",
    "        classifier = Network(output_size=NUM_CLASSES,\n",
    "                            softmax_sigmoid=\"softmax\").to(device)\n",
    "        classifiers.append(classifier)\n",
    "\n",
    "    cifar_dl = CIFAR100_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed, small_version=False)\n",
    "    train_loader, val_loader, test_loader = cifar_dl.get_data_loader()\n",
    "\n",
    "    parameters = list(allocation_system.parameters())\n",
    "    for classifier in classifiers:\n",
    "        parameters += list(classifier.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_loss = 100\n",
    "    best_test_system_accuracy = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(\"-\" * 20, f'Epoch {epoch}', \"-\" * 20)\n",
    "\n",
    "        train_moae_one_epoch(feature_extractor, classifiers, allocation_system, train_loader, optimizer, scheduler)\n",
    "        val_moae_accuracy, val_moae_loss = evaluate_moae_one_epoch(feature_extractor, classifiers, allocation_system, val_loader)\n",
    "        test_moae_accuracy, test_moae_loss = evaluate_moae_one_epoch(feature_extractor, classifiers, allocation_system, test_loader)\n",
    "\n",
    "        if val_moae_loss < best_val_system_loss:\n",
    "            best_val_system_loss = val_moae_loss\n",
    "            best_test_system_accuracy = test_moae_accuracy\n",
    "\n",
    "    print(f'Mixture of Artificial Experts Accuracy: {best_test_system_accuracy}\\n')\n",
    "    return best_test_system_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH43qjtg1PlF"
   },
   "source": [
    "Functions for Training and Evaluation of Mixture of Human Experts Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HbPGYX0h1PlG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_mohe_one_epoch(feature_extractor, allocation_system, train_loader, optimizer, scheduler, expert_fns):\n",
    "    # switch to train mode\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.train()\n",
    "\n",
    "    for i, (batch_input, batch_targets, batch_subclass_idxs) in enumerate(train_loader):\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        expert_batch_preds = np.empty((NUM_EXPERTS, len(batch_targets)))\n",
    "        for idx, expert_fn in enumerate(expert_fns):\n",
    "            expert_batch_preds[idx] = np.array(expert_fn(batch_subclass_idxs))\n",
    "\n",
    "        batch_features = feature_extractor(batch_input)\n",
    "        batch_outputs_allocation_system = allocation_system(batch_features)\n",
    "        \n",
    "        # compute and record loss\n",
    "        batch_loss = mixture_of_human_experts_loss(allocation_system_output=batch_outputs_allocation_system,\n",
    "                                                   human_expert_preds=expert_batch_preds, targets=batch_targets)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_LR_SCHEDULER:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "def evaluate_mohe_one_epoch(feature_extractor, allocation_system, data_loader, expert_fns):\n",
    "    feature_extractor.eval()\n",
    "    allocation_system.eval()\n",
    "\n",
    "    allocation_system_outputs = torch.tensor([]).to(device)\n",
    "    targets = torch.tensor([]).to(device)\n",
    "    subclass_idxs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_input, batch_targets, batch_subclass_idxs) in enumerate(data_loader):\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            batch_features = feature_extractor(batch_input)\n",
    "            batch_allocation_system_outputs = allocation_system(batch_features)\n",
    "            \n",
    "            allocation_system_outputs = torch.cat((allocation_system_outputs, batch_allocation_system_outputs))\n",
    "            targets = torch.cat((targets, batch_targets))\n",
    "            subclass_idxs.extend(batch_subclass_idxs)\n",
    "            \n",
    "            \n",
    "    expert_preds = np.empty((NUM_EXPERTS, len(targets)))\n",
    "    for idx, expert_fn in enumerate(expert_fns):\n",
    "        expert_preds[idx] = np.array(expert_fn(subclass_idxs))\n",
    "\n",
    "    # compute and record loss\n",
    "    mohe_loss = mixture_of_human_experts_loss(allocation_system_output=allocation_system_outputs,\n",
    "                                                   human_expert_preds=expert_preds, targets=targets.long())\n",
    "\n",
    "    allocation_system_outputs = allocation_system_outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    expert_preds = expert_preds.T\n",
    "    allocation_system_decisions = np.argmax(allocation_system_outputs, 1)\n",
    "    team_preds = expert_preds[range(len(expert_preds)), allocation_system_decisions.astype(int)]\n",
    "    mohe_accuracy = get_accuracy(team_preds, targets)\n",
    "\n",
    "    return mohe_accuracy, mohe_loss\n",
    "\n",
    "\n",
    "def run_mohe(seed, expert_fns):\n",
    "    print(f'Training Mixture of human experts baseline')\n",
    "\n",
    "    feature_extractor = Resnet().to(device)\n",
    "\n",
    "    allocation_system = Network(output_size=NUM_EXPERTS,\n",
    "                                 softmax_sigmoid=\"softmax\").to(device)\n",
    "\n",
    "    cifar_dl = CIFAR100_3_Split_Dataloader(train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, seed=seed, small_version=False)\n",
    "    train_loader, val_loader, test_loader = cifar_dl.get_data_loader()\n",
    "\n",
    "    parameters = allocation_system.parameters()\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LR, betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS * len(train_loader))\n",
    "\n",
    "    best_val_system_loss = 100\n",
    "    best_test_system_accuracy = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(\"-\" * 20, f'Epoch {epoch}', \"-\" * 20)\n",
    "\n",
    "        train_mohe_one_epoch(feature_extractor, allocation_system, train_loader, optimizer, scheduler, expert_fns)\n",
    "        val_mohe_accuracy, val_mohe_loss = evaluate_mohe_one_epoch(feature_extractor, allocation_system, val_loader, expert_fns)\n",
    "        test_mohe_accuracy, test_mohe_loss = evaluate_mohe_one_epoch(feature_extractor, allocation_system, test_loader, expert_fns)\n",
    "\n",
    "        if val_mohe_loss < best_val_system_loss:\n",
    "            best_val_system_loss = val_mohe_loss\n",
    "            best_test_system_accuracy = test_mohe_accuracy\n",
    "\n",
    "    print(f'Mixture of Human Experts Accuracy: {best_test_system_accuracy}\\n')\n",
    "    return best_test_system_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX69BkD8jcsy"
   },
   "source": [
    "# Run Experiment on Expert Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qSE8TX0BF-zc"
   },
   "outputs": [],
   "source": [
    "NUM_EXPERTS=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3fxGG2WV55q7"
   },
   "outputs": [],
   "source": [
    "best_expert_accuracies = {diversity_idx:[] for diversity_idx in range(11)}\n",
    "avg_expert_accuracies = {diversity_idx:[] for diversity_idx in range(11)}\n",
    "our_approach_accuracies = {diversity_idx:[] for diversity_idx in range(11)}\n",
    "our_approach_coverages = {diversity_idx:[] for diversity_idx in range(11)}\n",
    "jsf_accuracies = {diversity_idx:[] for diversity_idx in range(11)}\n",
    "jsf_coverages = {diversity_idx:[] for diversity_idx in range(11)}\n",
    "mohe_accuracies = {diversity_idx:[] for diversity_idx in range(11)}\n",
    "full_automation_accuracies = []\n",
    "moae_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcgtljuDjcsy",
    "outputId": "9aae0649-2f42-42b0-a743-2c474916f725",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 0\n",
      "----------------------------------------\n",
      "Diversity: 0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best Expert Accuracy: 0.905\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Average Expert Accuracy: 0.9048\n",
      "\n",
      "Team Performance Optimization with Our Approach\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [05:30<00:00, 165.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Earlystopping Results for Our Approach:\n",
      "\t System Accuracy: 0.9199\n",
      "\t System Loss: 0.2455103116276996\n",
      "\n",
      "\t Classifier Accuracy: 0.2732\n",
      "\t Classifier Task Subset Accuracy: 0.7561475409836066\n",
      "\t Classifier Coverage: 0.0976\n",
      "\n",
      "Team Performance Optimization with Joint Sparse Framework\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [04:56<00:00, 148.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Earlystopping Results for Joint Sparse Framework:\n",
      "\t System Accuracy: 0.906\n",
      "\t System Loss: 1.3715230885948453\n",
      "\n",
      "\t Classifier Accuracy: 0.6613\n",
      "\t Classifier Task Subset Accuracy: 0.631578947368421\n",
      "\t Classifier Coverage: 0.0019\n",
      "\n",
      "Training Mixture of human experts baseline\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-------------------- Epoch 1 --------------------\n",
      "-------------------- Epoch 2 --------------------\n",
      "Mixture of Human Experts Accuracy: 0.9046\n",
      "\n",
      "Training full automation baseline\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [04:02<00:00, 121.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Automation Accuracy: 0.6639\n",
      "\n",
      "Training Mixture of artificial experts baseline\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-------------------- Epoch 1 --------------------\n",
      "-------------------- Epoch 2 --------------------\n",
      "Mixture of Artificial Experts Accuracy: 0.6409\n",
      "\n",
      "----------------------------------------\n",
      "Seed: 1\n",
      "----------------------------------------\n",
      "Diversity: 0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best Expert Accuracy: 0.9051\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Average Expert Accuracy: 0.9045\n",
      "\n",
      "Team Performance Optimization with Our Approach\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [04:42<00:00, 141.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Earlystopping Results for Our Approach:\n",
      "\t System Accuracy: 0.8843\n",
      "\t System Loss: 0.2562796697270059\n",
      "\n",
      "\t Classifier Accuracy: 0.2788\n",
      "\t Classifier Task Subset Accuracy: 0.5828449653702718\n",
      "\t Classifier Coverage: 0.1877\n",
      "\n",
      "Team Performance Optimization with Joint Sparse Framework\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [04:44<00:00, 142.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Earlystopping Results for Joint Sparse Framework:\n",
      "\t System Accuracy: 0.906\n",
      "\t System Loss: 1.187657117843628\n",
      "\n",
      "\t Classifier Accuracy: 0.6186\n",
      "\t Classifier Task Subset Accuracy: 0\n",
      "\t Classifier Coverage: 0.0\n",
      "\n",
      "Training Mixture of human experts baseline\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-------------------- Epoch 1 --------------------\n",
      "-------------------- Epoch 2 --------------------\n",
      "Mixture of Human Experts Accuracy: 0.9031\n",
      "\n",
      "Training full automation baseline\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [03:59<00:00, 119.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Automation Accuracy: 0.6672\n",
      "\n",
      "Training Mixture of artificial experts baseline\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "-------------------- Epoch 1 --------------------\n",
      "-------------------- Epoch 2 --------------------\n",
      "Mixture of Artificial Experts Accuracy: 0.6783\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seed in range(2):\n",
    "    print(f'Seed: {seed}')\n",
    "    print(\"-\"*40)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    subclass_idxs = list(range(0,100))\n",
    "    random.shuffle(subclass_idxs)\n",
    "\n",
    "    for diversity_idx in range(11):\n",
    "        print(f'Diversity: {diversity_idx}')\n",
    "\n",
    "        expert_strengths = [subclass_idxs[0:90], subclass_idxs[0+diversity_idx:90+diversity_idx]]\n",
    "        expert_fns = []\n",
    "        for i, strengths in enumerate(expert_strengths):\n",
    "            cifar100_expert = Cifar100Expert(strengths=strengths)\n",
    "            expert_fns.append(cifar100_expert.predict)\n",
    "\n",
    "        best_expert_accuracy = get_accuracy_of_best_expert(seed, expert_fns)\n",
    "        best_expert_accuracies[diversity_idx].append(best_expert_accuracy)\n",
    "        \n",
    "        avg_expert_accuracy = get_accuracy_of_average_expert(seed, expert_fns)\n",
    "        avg_expert_accuracies[diversity_idx].append(avg_expert_accuracy)\n",
    "\n",
    "        our_approach_accuracy, our_approach_coverage = run_team_performance_optimization(\"Our Approach\", seed, expert_fns)\n",
    "        our_approach_accuracies[diversity_idx].append(our_approach_accuracy)\n",
    "        our_approach_coverages[diversity_idx].append(our_approach_coverage)\n",
    "        \n",
    "        jsf_accuracy, jsf_coverage = run_team_performance_optimization(\"Joint Sparse Framework\", seed, expert_fns)\n",
    "        jsf_accuracies[diversity_idx].append(jsf_accuracy)\n",
    "        jsf_coverages[diversity_idx].append(jsf_coverage)\n",
    "\n",
    "        mohe_accuracy = run_mohe(seed, expert_fns)\n",
    "        mohe_accuracies[diversity_idx].append(mohe_accuracy)\n",
    "\n",
    "    full_automation_accuracy = run_full_automation(seed)\n",
    "    full_automation_accuracies.append(full_automation_accuracy)\n",
    "\n",
    "    moae_accuracy = run_moae(seed)\n",
    "    moae_accuracies.append(moae_accuracy)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pyRl5EKjcsy",
    "outputId": "a87668fd-83a8-40fc-9407-ea60dbf857f5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity    Method           Accuracy\n",
      "-----------  ---------------  ------------------\n",
      "--------     Full Automation  0.6655500000000001\n",
      "--------     MOAE             0.6596\n",
      "--------     --------         --------\n",
      "0            Best Expert      0.90505\n",
      "0            Random Expert    0.90465\n",
      "0            Our Approach     0.9021\n",
      "0            JSF              0.906\n",
      "0            MOHE             0.90385\n",
      "--------     --------         --------\n",
      "1            Best Expert      nan\n",
      "1            Random Expert    nan\n",
      "1            Our Approach     nan\n",
      "1            JSF              nan\n",
      "1            MOHE             nan\n",
      "--------     --------         --------\n",
      "2            Best Expert      nan\n",
      "2            Random Expert    nan\n",
      "2            Our Approach     nan\n",
      "2            JSF              nan\n",
      "2            MOHE             nan\n",
      "--------     --------         --------\n",
      "3            Best Expert      nan\n",
      "3            Random Expert    nan\n",
      "3            Our Approach     nan\n",
      "3            JSF              nan\n",
      "3            MOHE             nan\n",
      "--------     --------         --------\n",
      "4            Best Expert      nan\n",
      "4            Random Expert    nan\n",
      "4            Our Approach     nan\n",
      "4            JSF              nan\n",
      "4            MOHE             nan\n",
      "--------     --------         --------\n",
      "5            Best Expert      nan\n",
      "5            Random Expert    nan\n",
      "5            Our Approach     nan\n",
      "5            JSF              nan\n",
      "5            MOHE             nan\n",
      "--------     --------         --------\n",
      "6            Best Expert      nan\n",
      "6            Random Expert    nan\n",
      "6            Our Approach     nan\n",
      "6            JSF              nan\n",
      "6            MOHE             nan\n",
      "--------     --------         --------\n",
      "7            Best Expert      nan\n",
      "7            Random Expert    nan\n",
      "7            Our Approach     nan\n",
      "7            JSF              nan\n",
      "7            MOHE             nan\n",
      "--------     --------         --------\n",
      "8            Best Expert      nan\n",
      "8            Random Expert    nan\n",
      "8            Our Approach     nan\n",
      "8            JSF              nan\n",
      "8            MOHE             nan\n",
      "--------     --------         --------\n",
      "9            Best Expert      nan\n",
      "9            Random Expert    nan\n",
      "9            Our Approach     nan\n",
      "9            JSF              nan\n",
      "9            MOHE             nan\n",
      "--------     --------         --------\n",
      "10           Best Expert      nan\n",
      "10           Random Expert    nan\n",
      "10           Our Approach     nan\n",
      "10           JSF              nan\n",
      "10           MOHE             nan\n",
      "--------     --------         --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "table_list = []\n",
    "\n",
    "mean_full_automation_accuracy = np.mean(full_automation_accuracies)\n",
    "mean_moae_accuracy = np.mean(moae_accuracies)\n",
    "table_list.append(['--------', 'Full Automation', mean_full_automation_accuracy])\n",
    "table_list.append(['--------', 'MOAE', mean_moae_accuracy])\n",
    "table_list.append(['--------', '--------', '--------'])\n",
    "\n",
    "for diversity_idx in range(11):\n",
    "  mean_best_expert_accuracy = np.mean(best_expert_accuracies[diversity_idx])\n",
    "  table_list.append([diversity_idx, 'Best Expert', mean_best_expert_accuracy])\n",
    "\n",
    "  mean_avg_expert_accuracy = np.mean(avg_expert_accuracies[diversity_idx])\n",
    "  table_list.append([diversity_idx, 'Random Expert', mean_avg_expert_accuracy])\n",
    "\n",
    "  mean_our_approach_accuracy = np.mean(our_approach_accuracies[diversity_idx])\n",
    "  table_list.append([diversity_idx, 'Our Approach', mean_our_approach_accuracy])\n",
    "\n",
    "  mean_jsf_accuracy = np.mean(jsf_accuracies[diversity_idx])\n",
    "  table_list.append([diversity_idx, 'JSF', mean_jsf_accuracy])\n",
    "\n",
    "  mean_mohe_accuracy = np.mean(mohe_accuracies[diversity_idx])\n",
    "  table_list.append([diversity_idx, 'MOHE', mean_mohe_accuracy])\n",
    "\n",
    "  table_list.append(['--------', '--------', '--------'])\n",
    "\n",
    "\n",
    "print(tabulate(table_list, headers=['Diversity', 'Method', 'Accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfUsTBSGGIHf"
   },
   "source": [
    "#Run Experiment on Number of Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4lF-jE9GBRO"
   },
   "outputs": [],
   "source": [
    "best_expert_accuracies = {exp_idx:[] for exp_idx in range(NUM_EXPERTS)}\n",
    "avg_expert_accuracies = {exp_idx:[] for exp_idx in range(NUM_EXPERTS)}\n",
    "our_approach_accuracies = {exp_idx:[] for exp_idx in range(NUM_EXPERTS)}\n",
    "our_approach_coverages = {exp_idx:[] for exp_idx in range(NUM_EXPERTS)}\n",
    "jsf_accuracies = {exp_idx:[] for exp_idx in range(NUM_EXPERTS)}\n",
    "jsf_coverages = {exp_idx:[] for exp_idx in range(NUM_EXPERTS)}\n",
    "mohe_accuracies = {exp_idx:[] for exp_idx in range(NUM_EXPERTS)}\n",
    "full_automation_accuracies = []\n",
    "moae_accuracies = []\n",
    "\n",
    "for seed in range(1):\n",
    "    print(f'Seed: {seed}')\n",
    "    print(\"-\"*40)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    subclass_idxs = list(range(0,100))\n",
    "    expert_strengths = []\n",
    "    for _ in range(10):\n",
    "        strengths = random.sample(subclass_idxs, 60)\n",
    "        expert_strengths.append(strengths)\n",
    "\n",
    "    for num_experts in range(2,11):\n",
    "        print(f'Number of Experts: {num_experts}')\n",
    "        NUM_EXPERTS = num_experts\n",
    "\n",
    "        expert_fns = []\n",
    "        for i, strengths in enumerate(expert_strengths[:num_experts]):\n",
    "            cifar100_expert = Cifar100Expert(strengths=strengths)\n",
    "            expert_fns.append(cifar100_expert.predict)\n",
    "\n",
    "        best_expert_accuracy = get_accuracy_of_best_expert(seed, expert_fns)\n",
    "        best_expert_accuracies[num_experts].append(best_expert_accuracy)\n",
    "        \n",
    "        avg_expert_accuracy = get_accuracy_of_average_expert(seed, expert_fns)\n",
    "        avg_expert_accuracies[num_experts].append(avg_expert_accuracy)\n",
    "\n",
    "        our_approach_accuracy, our_approach_coverage = run_team_performance_optimization(\"Our Approach\", seed, expert_fns)\n",
    "        our_approach_accuracies[num_experts].append(our_approach_accuracy)\n",
    "        \n",
    "        jsf_accuracy, jsf_coverage = run_team_performance_optimization(\"Joint Sparse Framework\", seed, expert_fns)\n",
    "        jsf_accuracies[num_experts].append(jsf_accuracy)\n",
    "\n",
    "        mohe_accuracy = run_mohe(seed, expert_fns)\n",
    "        mohe_accuracies[num_experts].append(mohe_accuracy)\n",
    "\n",
    "    full_automation_accuracy = run_full_automation(seed)\n",
    "    full_automation_accuracies.append(full_automation_accuracy)\n",
    "\n",
    "    moae_accuracy = run_moae(seed)\n",
    "    moae_accuracies.append(moae_accuracy)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7E1IV-KFSaG"
   },
   "outputs": [],
   "source": [
    "table_list = []\n",
    "\n",
    "mean_full_automation_accuracy = np.mean(full_automation_accuracies)\n",
    "mean_moae_accuracy = np.mean(moae_accuracies)\n",
    "table_list.append(['--------', 'Full Automation', mean_full_automation_accuracy])\n",
    "table_list.append(['--------', 'MOAE', mean_moae_accuracy])\n",
    "table_list.append(['--------', '--------', '--------'])\n",
    "\n",
    "for num_experts in range(2,11):\n",
    "  mean_best_expert_accuracy = np.mean(best_expert_accuracies[num_experts])\n",
    "  table_list.append([num_experts, 'Best Expert', mean_best_expert_accuracy])\n",
    "\n",
    "  mean_avg_expert_accuracy = np.mean(avg_expert_accuracies[num_experts])\n",
    "  table_list.append([num_experts, 'Random Expert', mean_avg_expert_accuracy])\n",
    "\n",
    "  mean_our_approach_accuracy = np.mean(our_approach_accuracies[num_experts])\n",
    "  table_list.append([num_experts, 'Our Approach', mean_our_approach_accuracy])\n",
    "\n",
    "  mean_jsf_accuracy = np.mean(jsf_accuracies[num_experts])\n",
    "  table_list.append([num_experts, 'JSF', mean_jsf_accuracy])\n",
    "\n",
    "  mean_mohe_accuracy = np.mean(mohe_accuracies[num_experts])\n",
    "  table_list.append([num_experts, 'MOHE', mean_mohe_accuracy])\n",
    "\n",
    "  table_list.append(['--------', '--------', '--------'])\n",
    "\n",
    "\n",
    "print(tabulate(table_list, headers=['Number of Experts', 'Method', 'Accuracy']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR_100.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
